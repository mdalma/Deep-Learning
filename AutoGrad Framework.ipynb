{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c39VZTU6NZZn"
      },
      "source": [
        "# Creating our own Auto Differentiation (AutoGrad) framework\n",
        "\n",
        "In this practical exercise we will build our own, very simple, Auto-differentiation (or AutoGrad) framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JocJ_LzjNZZr"
      },
      "source": [
        "## Coding the framework\n",
        "\n",
        "### Step 1: Define a class for our variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft6a0F4BNZZt"
      },
      "source": [
        "The key idea is that we will define our own class of `Variable` which is basically the same as a scalar (a number). So our class is created by passing it a `value`, and it stores this value internally.\n",
        "\n",
        "But apart from being a placeholder for a number, we also want to keep track of the way every variable was created.\n",
        "\n",
        "For example, if a variable $c$ is the result of the addition of two variables $a$ and $b$: $c = a + b$, then we would say that $a$ and $b$ are \"parent\" variables of $c$, and $c$ is their \"child\". The way $c$ was created was by adding these two parent variables together.\n",
        "\n",
        "So apart from the value of the variable, we will also have to keep track of the parents, and on how each of them \"contributes\" to the creation of the variable - this is described by the local derivative associated with each of the parents, that tells us how a change in each of the parent variables translates into a change in the child variable.\n",
        "\n",
        "This is important in order to implement our backwards pass. Each parent defines a \"route\" through which the gradients coming into this variable will have to flow through. So we will define a list of `gradRoutes` that will contain the list of parent variables and their corresponding local derivatives. A variable created directly (not resulting by any operation over existing variables) will have an empty `gradRoutes`.\n",
        "\n",
        "Finally, we want each variable of ours to keep also the value of gradient of the quantity we are interested in with respect to that variable. We will create a placeholder for that as well, called `grad`. As seen before, this will accummulate the gradients that are backpropagated from the children of this variable when we implement the backpropagation algorithm. So we will initialise it to zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vJ_a2SxcNZZu"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ashv4NJ1NZZx"
      },
      "outputs": [],
      "source": [
        "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
        "        self.grad = 0.0        \n",
        "        \n",
        "    def __str__(self):\n",
        "        return 'Value: {self.value}'.format(self=self)        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d2_671bNZZz"
      },
      "source": [
        "Apart from the `__init__()` function which stores the value passed to our class and initialises the `gradRoutes` and `grad` member variables, we have also overloaded the function that python uses to convert a class into a string representation: `__str__()`. This will allow us to print our class.\n",
        "\n",
        "We cannot do much yet with this class, apart from storing values into our variables and printing them out. Let's try this out. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVYNEDHXNZZ0",
        "outputId": "c921acc1-e902-4f2c-d58f-8a0c3e4230ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value: 4.3\n",
            "Value: 5.2\n"
          ]
        }
      ],
      "source": [
        "a = Variable(4.3)\n",
        "b = Variable(5.2)\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O2edUeeNZZ1"
      },
      "source": [
        "### Step 2: Define operations over our variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpn39ufGNZZ1"
      },
      "source": [
        "The next step would be to implement operations on our variables. Let's first define the operations for addition and multiplication.\n",
        "\n",
        "These will be functions that take two variables as input and produce a new (child) variable with a value equal to the sum or the product of the two inputs. Apart from the forward pass though, we should keep track of how this new variable was created: the two parent variables, and their corresponding local derivatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5RIY225aNZZ3"
      },
      "outputs": [],
      "source": [
        "def vAdd(A: Variable, B: Variable): # Addition\n",
        "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
        "          \n",
        "    #keep track of the parent variables, and of the local derivative associated with each one\n",
        "    result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
        "    result.gradRoutes.append((B, 1)) # dresult / dB = 1\n",
        "    \n",
        "    return result\n",
        "    \n",
        "def vMul(A: Variable, B: Variable): # Addition\n",
        "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a * b\n",
        "          \n",
        "    #keep track of the parent variables, and of the local derivative associated with each one\n",
        "    result.gradRoutes.append((A, B.value)) # dresult / dA = B\n",
        "    result.gradRoutes.append((B, A.value)) # dresult / dB = A\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0hcvOC4NZZ4"
      },
      "source": [
        "So for example to calculate $d = (a + b) * c$ we first need to calculate $(a + b)$ and then mutiply the result with $c$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB1TwxvkNZZ4",
        "outputId": "c348ef22-ad17-4712-f00f-94386b83d1a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value: 20\n"
          ]
        }
      ],
      "source": [
        "a = Variable(2) # a = 2\n",
        "b = Variable(3) # b = 3\n",
        "c = Variable(4) # c = 4\n",
        "\n",
        "#d = (a + b) * c = 20\n",
        "d = vMul(vAdd(a, b), c)\n",
        "\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1c-FjgYNZZ5"
      },
      "source": [
        "### Step 3: Implement the backpropagation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6-F-YtYNZZ5"
      },
      "source": [
        "The final step is to implement the backpropagation function. This starts with a child variable, and backpropagates gradients through the routes defined recursively. It uses the two rules that we saw before:\n",
        "\n",
        "- Accumulate the incoming gradients from the different grad routes that lead to a node (a variable). Each of the incoming gradients describe a different way in which the node affects the quantity of interest, so this sum will be the final gradient for the node\n",
        "- Multiply every incoming gradient with each of the local derivatives corresponding to parent variables (this would be the application of the chain rule), and continue the backpropagation through the corresponding route (for each of the parent variables)\n",
        "\n",
        "We update the `Variable` class accordingly. We also update the `__str___()` function to include also gradient information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-9Hf0qFWNZZ6"
      },
      "outputs": [],
      "source": [
        "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
        "        self.grad = 0.0\n",
        "    \n",
        "    def backProp(self, route_val = 1.0):\n",
        "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
        "        self.grad += route_val\n",
        "                \n",
        "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
        "        for variable, local_derivative_value in self.gradRoutes:\n",
        "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
        "            variable.backProp(local_derivative_value * route_val)\n",
        "    \n",
        "    def __str__(self):\n",
        "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcOoOOx9NZZ7"
      },
      "source": [
        "<font color=blue>**Question:** Why did we set the default value of route_val equal to 1.0?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQF0siA0NZZ8"
      },
      "source": [
        "----\n",
        "\n",
        "*Answer*\n",
        "\n",
        "\n",
        "---\n",
        "Si no li passem el paràmetre el que farem és incrementar el grad en una unitat. En relació a la pregunta, *route_val* és 1 perquè la derivada de la funció identitat (derivada de f respecte a f és 1). Per cada ruta cridarem *backprop* de cada un dels pares, però ara si passaré el paràmetre de la derivada local (en cas de suma 1) i el multiplico pel *route_val*.\n",
        "\n",
        "En altres paraules, perquè la derivada parcial d'una variable en funció d'ella mateixa sempre resulta en 1. Aquest procés es repetirà per cada variable, per tant, podem simplificar el procés i tenir precalculat aquest valor per defecte.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhrgpZjDNZZ8"
      },
      "source": [
        "This should be all. If we want to calculate the derivative of the result with respect to any of the variables that participated in the calculation, we just need to call backprop on the result, and then read the derivatives out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0j3yqfhNZZ8",
        "outputId": "64a952a0-5a41-4f41-8f30-0d048ae60c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result = 20\n",
            "The derivative of the result with respect to a is: 4.0\n",
            "The derivative of the result with respect to b is: 4.0\n",
            "The derivative of the result with respect to c is: 5.0\n"
          ]
        }
      ],
      "source": [
        "a = Variable(2)           # a = 2\n",
        "b = Variable(3)           # b = 3\n",
        "c = Variable(4)           # c = 4\n",
        "res = vMul(vAdd(a, b), c) # res = (a + b) * c = 20\n",
        "\n",
        "print(\"Result =\", res.value)\n",
        "\n",
        "# Call backprop on the result\n",
        "res.backProp()\n",
        "\n",
        "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
        "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
        "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
        "print(\"The derivative of the result with respect to c is:\", c.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EANRi2ltNZZ9"
      },
      "source": [
        "In the following example, variable $a$ affects the result through two different routes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLkIiN1ANZZ-",
        "outputId": "5a1baaf0-255e-44ea-be78-5e5c1ad32e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result = 28\n",
            "The derivative of the result with respect to a is: 11.0\n",
            "The derivative of the result with respect to b is: 4.0\n",
            "The derivative of the result with respect to c is: 4.0\n"
          ]
        }
      ],
      "source": [
        "a = Variable(4)  # a = 4\n",
        "b = Variable(3)  # b = 3\n",
        "c = vAdd(a, b)   # c = 4 + 3\n",
        "res = vMul(a, c) # res = a * c = 28\n",
        "\n",
        "print(\"Result =\", res.value)\n",
        "\n",
        "# Call backprop on the result\n",
        "res.backProp()\n",
        "\n",
        "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
        "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
        "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
        "# Also for intermediate results\n",
        "print(\"The derivative of the result with respect to c is:\", c.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa7NSf4BNZZ_"
      },
      "source": [
        "<font color=blue>**Question:** Can you now use this setup to calculate the derivative of $c$ with respect to $a$ and $b$?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "L'únic canvi que hem de fer en el nostre codi és fer la propagació de la variable C en compte de res. A partir d'aquí, podem saber el gradient respecte a un altre variable amb .grad.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "P4MHvlCi-6hV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1OYLnjXNZZ_",
        "outputId": "9e3b3c0e-5e32-4c3f-bfdb-941710065a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C = 7\n",
            "The derivative of the c with respect to a is: 1.0\n",
            "The derivative of the c with respect to b is: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Your Code Here\n",
        "a = Variable(4)  # a = 4\n",
        "b = Variable(3)  # b = 3\n",
        "c = vAdd(a, b)   # c = 4 + 3\n",
        "res = vMul(a, c) # res = a * c = 28\n",
        "\n",
        "print(\"C =\", c.value)\n",
        "\n",
        "# Call backprop on the result\n",
        "c.backProp()\n",
        "\n",
        "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
        "print(\"The derivative of the c with respect to a is:\", a.grad)\n",
        "print(\"The derivative of the c with respect to b is:\", b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLiPDyOINZaA"
      },
      "source": [
        "## Final touches\n",
        "\n",
        "If you understood how this works up to here, then you should be already good to go. But since we want to use our auto grad to do some practical work, we will continue working on it a bit, to make it a bit more usable and complete it with more operations. Many of the subsequent steps are quite \"engineering\" in nature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjqWyWR9NZaB"
      },
      "source": [
        "### Improving usability: overloading operators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2urAc303NZaB"
      },
      "source": [
        "Of course this is still highly incomplete, very inefficient and not very usable. Lets first improve a usability issue. Instead of having to call different functions for the operations like `res = vMul(a, c)`, we would like to be able to directly write them down like `res = a * b`. To achieve this, we should overload [Python's special functions for operator overloading](https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types).\n",
        "\n",
        "Here's how to do this for the addition and multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Nz5f1ig-NZaL"
      },
      "outputs": [],
      "source": [
        "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
        "        self.grad = 0.0\n",
        "    \n",
        "    def backProp(self, route_val = 1.0):\n",
        "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
        "        self.grad += route_val\n",
        "                \n",
        "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
        "        for variable, local_derivative_value in self.gradRoutes:\n",
        "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
        "            variable.backProp(local_derivative_value * route_val)\n",
        "            \n",
        "    def __add__(self, b):\n",
        "        return vAdd(self, b)\n",
        "        \n",
        "    def __mul__(self, b):\n",
        "        return vMul(self, b)            \n",
        "    \n",
        "    def __str__(self):\n",
        "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26GkgBBmNZaM",
        "outputId": "29bba2e9-cb31-4651-b213-9a5af1697b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result = 28\n",
            "The derivative of the result with respect to a is: 11.0\n",
            "The derivative of the result with respect to b is: 4.0\n",
            "The derivative of the result with respect to c is: 4.0\n"
          ]
        }
      ],
      "source": [
        "a = Variable(4)  # a = 4\n",
        "b = Variable(3)  # b = 3\n",
        "c = a + b        # c = 4 + 3\n",
        "res = a * c      # res = a * c = 28\n",
        "\n",
        "print(\"Result =\", res.value)\n",
        "\n",
        "# Call backprop on the result\n",
        "res.backProp()\n",
        "\n",
        "# Now all variables should contain in their \"grad\" the derivative d(res) / d(variable)\n",
        "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
        "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
        "# Also for intermediate results\n",
        "print(\"The derivative of the result with respect to c is:\", b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMZf51KJNZaM"
      },
      "source": [
        "### Zeroing gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdXK84UJNZaN"
      },
      "source": [
        "A last thing to note is that once we call `backProp`, our gradients are calculated and our variables are now \"dirty\" in the sense that if we call backprop again, the new result will be added to the previous one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TzJyfRIWNZaN",
        "outputId": "54242a36-c0e6-459f-cdf7-d06400109bf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The derivative of the result with respect to a is: 11.0\n",
            "The derivative of the result with respect to b is: 4.0\n",
            "Second time\n",
            "The derivative of the result with respect to a is: 22.0\n",
            "The derivative of the result with respect to b is: 8.0\n"
          ]
        }
      ],
      "source": [
        "a = Variable(4)   # a = 4\n",
        "b = Variable(3)   # b = 3\n",
        "res = (a + b) * a # res = a * c = 28\n",
        "\n",
        "# Call backprop on the result\n",
        "res.backProp()\n",
        "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
        "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
        "\n",
        "# Call backprop on the result once more\n",
        "print(\"Second time\")\n",
        "res.backProp()\n",
        "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
        "print(\"The derivative of the result with respect to b is:\", b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQUMBlQmNZaN"
      },
      "source": [
        "This will actually turn out to be quite useful, e.g. when we want to accumulate weight gradients over different samples in our learning loops (see next), but we need a way to control it.\n",
        "\n",
        "To avoid this, we should reset the gradients to zero before we call `backProp` again. We can do it one by one for every variable, but we will also implement a function that does this recursively from the child node we backProped from all the way to the parents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jA4c3ZByNZaN"
      },
      "outputs": [],
      "source": [
        "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
        "        self.grad = 0.0\n",
        "    \n",
        "    def backProp(self, route_val = 1.0):\n",
        "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
        "        self.grad += route_val\n",
        "                \n",
        "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
        "        for variable, local_derivative_value in self.gradRoutes:\n",
        "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
        "            variable.backProp(local_derivative_value * route_val)\n",
        "\n",
        "    def zeroGrad(self):\n",
        "        self.grad = 0.0\n",
        "        \n",
        "    def zeroGradsRecursively(self):\n",
        "        self.zeroGrad()\n",
        "        for variable, _ in self.gradRoutes:\n",
        "            variable.zeroGradsRecursively()\n",
        "            \n",
        "    def __add__(self, b):\n",
        "        return vAdd(self, b)\n",
        "        \n",
        "    def __mul__(self, b):\n",
        "        return vMul(self, b)           \n",
        "    \n",
        "    def __str__(self):\n",
        "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSfWU4_CNZaO",
        "outputId": "82e222c5-8df1-4b04-b497-ff3eb58b6aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The derivative of the result with respect to a is: 11.0\n",
            "The derivative of the result with respect to b is: 4.0\n",
            "Second time\n",
            "The derivative of the result with respect to a is: 11.0\n",
            "The derivative of the result with respect to b is: 4.0\n"
          ]
        }
      ],
      "source": [
        "a = Variable(4)   # a = 4\n",
        "b = Variable(3)   # b = 3\n",
        "res = (a + b) * a # res = a * c = 28\n",
        "\n",
        "# Call backprop on the result\n",
        "res.backProp()\n",
        "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
        "print(\"The derivative of the result with respect to b is:\", b.grad)\n",
        "\n",
        "# Zero gradients\n",
        "res.zeroGradsRecursively()\n",
        "\n",
        "# Call backprop on the result once more\n",
        "print(\"Second time\")\n",
        "res.backProp()\n",
        "print(\"The derivative of the result with respect to a is:\", a.grad)\n",
        "print(\"The derivative of the result with respect to b is:\", b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ59_MNNNZaP"
      },
      "source": [
        "## More Improvements - Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd7kQhUONZaP"
      },
      "source": [
        "There are number of ways we can improve our simple network. The most important is probably being able to work with vectors and matrices - we will look into this in the next notebook. Before that, there are still a lot of things to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYa9K1_tNZaP"
      },
      "source": [
        "---\n",
        "### <font color=blue>Excercise 1:</font>\n",
        "    \n",
        "<font color=blue>We usually do not require gradients for all our variables. If we could indicate which variables require gradients, then we could only keep track of the routes that lead to these variables only and drop all the rest. This would be a huge improvement in resources and speed (number of calculations). Can you add this functionality?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Com no totes les variables requeriran saber els gradients, podem agilitzar els càlculs calculant només el gradient quan sigui necessari. Per fer-ho, el que farem és afegir un atribut en el constructor (*requiresGrad*). Aquest estarà a False per defecte, per tant, és tasca del programador indicar quines són les variables que requereixen gradients.\n",
        "\n",
        "En el següent exercici, veurem com també haurem de fer canvis en les operacions (multiplicació, suma, divisió, etc.). Com no totes les variables requereixen gradient, haurem de controlar amb un condicional quan una variable farà les operacions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dWEYoMWaAAvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q8AzmKPpNZaP"
      },
      "outputs": [],
      "source": [
        "# Your Code Here\n",
        "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
        "    def __init__(self, value, requiresGrad = False):\n",
        "        self.value = value\n",
        "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
        "        self.grad = 0.0\n",
        "        self.requiresGrad = requiresGrad\n",
        "    \n",
        "    def backProp(self, route_val = 1.0):\n",
        "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
        "        self.grad += route_val\n",
        "                \n",
        "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
        "        for variable, local_derivative_value in self.gradRoutes:\n",
        "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
        "            variable.backProp(local_derivative_value * route_val)\n",
        "    \n",
        "    def zeroGrad(self):\n",
        "        self.grad = 0.0\n",
        "        \n",
        "    def zeroGradsRecursively(self):\n",
        "        self.zeroGrad()\n",
        "        for variable, _ in self.gradRoutes:\n",
        "            variable.zeroGradsRecursively()\n",
        "            \n",
        "    def __add__(self, b):\n",
        "        return vAdd(self, b)\n",
        "        \n",
        "    def __mul__(self, b):\n",
        "        return vMul(self, b)           \n",
        "    \n",
        "    def __str__(self):\n",
        "        return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dEVJQOvNZaQ"
      },
      "source": [
        "---\n",
        "### <font color=blue>Excercise 2:</font>\n",
        "    \n",
        "<font color=blue>We obviously need to implement more functions - implement the following functions:\n",
        "- Subtraction\n",
        "- Raising to a power\n",
        "- Division\n",
        "- Unary negation\n",
        "- The (natural) exponential function exp(x)\n",
        "- ... any other function you might want</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Implementem les operacions a través de funcions tenint en compte que no totes les variables requereixen gradient.\n",
        "\n",
        "Per tant, en el cas que la variable requereixi gradient, el que farem és afegir la ruta del gradient calculant-ho de forma manual. Notem que podríem optimitzar això creant una funció dins d'aquests mètodes perquè calculi el gradient de forma automàtica, això ho farem més endavant.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GtIBgKCEAf5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vAdd(A: Variable, B: Variable): # Addition\n",
        "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
        "    \n",
        "    if (A.requiresGrad):\n",
        "      #keep track of the parent variables, and of the local derivative associated with each one\n",
        "      result.requiresGrad = True\n",
        "      result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
        "\n",
        "    if (B.requiresGrad):\n",
        "      result.requiresGrad = True\n",
        "      result.gradRoutes.append((B, 1)) # dresult / dB = 1\n",
        "    \n",
        "    return result\n",
        "    \n",
        "def vMul(A: Variable, B: Variable): # Addition\n",
        "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a * b\n",
        "\n",
        "    if (A.requiresGrad):    \n",
        "      #keep track of the parent variables, and of the local derivative associated with each one\n",
        "      result.requiresGrad = True\n",
        "      result.gradRoutes.append((A, B.value)) # dresult / dA = B\n",
        "\n",
        "    if (B.requiresGrad):\n",
        "      result.requiresGrad = True\n",
        "      result.gradRoutes.append((B, A.value)) # dresult / dB = A\n",
        "    \n",
        "    return result\n",
        "  \n",
        "\n",
        "def vPow(A: Variable, exponent: float):\n",
        "    result = Variable(np.float_power(A.value, exponent))\n",
        "\n",
        "    if (A.requiresGrad):   \n",
        "      result.requiresGrad = True\n",
        "      result.grad = 0.0\n",
        "      result.gradRoutes.append((A, exponent*np.float_power(A.value, exponent -1)))\n",
        "    \n",
        "\n",
        "    return result\n",
        "\n",
        "def vExp(A: Variable):\n",
        "    result = Variable(np.exp(A.value))\n",
        "\n",
        "    if (A.requiresGrad):   \n",
        "      result.requiresGrad = True\n",
        "      result.grad = 0.0\n",
        "      result.gradRoutes.append((A, np.exp(A.value)))\n",
        "\n",
        "    return result\n",
        "\n",
        "def vLog(A: Variable):\n",
        "    result = Variable(np.log(A.value))\n",
        "\n",
        "    if (A.requiresGrad):   \n",
        "      result.requiresGrad = True\n",
        "      result.grad = 0.0\n",
        "      result.gradRoutes.append((A, 1/A.value))\n",
        "\n",
        "def vNeg(A: Variable):\n",
        "    result = Variable(A.value * (-1))\n",
        "\n",
        "    if (A.requiresGrad):   \n",
        "      result.requiresGrad = True\n",
        "      result.grad = 0.0\n",
        "      result.gradRoutes.append((A, A.value * (-1)))"
      ],
      "metadata": {
        "id": "KdM6ow6mWDgv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZtwY5BduNZaQ"
      },
      "outputs": [],
      "source": [
        "# Your Code Here\n",
        "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
        "    def __init__(self, value, requiresGrad = False):\n",
        "        self.value = value\n",
        "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
        "        self.grad = 0.0\n",
        "        self.requiresGrad = requiresGrad\n",
        "    \n",
        "    def backProp(self, route_val = 1.0):\n",
        "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
        "        self.grad += route_val\n",
        "                \n",
        "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
        "        for variable, local_derivative_value in self.gradRoutes:\n",
        "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
        "            variable.backProp(local_derivative_value * route_val)\n",
        "    \n",
        "    def zeroGrad(self):\n",
        "        self.grad = 0.0\n",
        "        \n",
        "    def zeroGradsRecursively(self):\n",
        "        self.zeroGrad()\n",
        "        for variable, _ in self.gradRoutes:\n",
        "            variable.zeroGradsRecursively()\n",
        "            \n",
        "    def __add__(self, b):\n",
        "        return vAdd(self, b)\n",
        "    \n",
        "    def __sub__(self, b):\n",
        "        return vAdd(self, vMul(b, Variable(-1.0)))\n",
        "  \n",
        "    def __mul__(self, b):\n",
        "        return vMul(self, b)        \n",
        "\n",
        "    def __pow__(self, exponent):\n",
        "        return vPow(self, exponent)\n",
        "\n",
        "    def __truediv__(self, b):\n",
        "        return vMul(self, b**(-1.0))\n",
        "\n",
        "    def __neg__(self):\n",
        "        return self * Variable(-1)\n",
        "\n",
        "    def __str__(self):\n",
        "        if self.requiresGrad:\n",
        "          return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)\n",
        "        else:\n",
        "          return 'Value: {self.value}, Gradient not required'.format(self=self)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uHm9NoXNZaQ"
      },
      "source": [
        "---\n",
        "### <font color=blue>Excercise 3:</font>\n",
        "    \n",
        "<font color=blue>Our operations currently accept only instances of our Variable class as inputs. So, if you wanted to calculate `a = b * 2` where `b` is an instance of our variable class and `2` is just a numerical constant you would get an error as our framework does not know how to multiply a `Variable` with a number. You should instead write `a = b * Variable(2)` to achieve this.</font>\n",
        "\n",
        "<font color=blue>Improve further the usability of our framework by allowing our functions to mix numbers and Variables in the same operation (look also into the overloads of the [reflected operands in python](https://docs.python.org/3/reference/datamodel.html#object.__radd__))</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Per aquest exercici caldrà implementar dues coses, la primera serà per evitar l'error que apareixerà quan intentem fer una operació amb una variable de tipus *Variable* i un altre tipus (int). La segona serà per poder fer operacions amb diferent ordre. Això ho haurem de fer per poder fer, per exemple: (Variable + 2) i (2 + Variable).\n",
        "\n",
        "Per solucionar el primer problema definirem un mètode que s'asseguri que les operacions que estem intentant fer siguin del mateix tipus de dada. Si les dues dades són del tipus Variable no caldrà fer res, doncs es podrà operar entre elles. En cas que una d'elles sigui de tipus int, es cridarà al constructor de Variable() passant-li aquesta dada i convertint-la, així les dues seran del mateix tipus i podrem fer operacions. En cas que la dada no sigui numèrica, per exemple str, es tornarà un error indicant al programador el que està passant. Cridarem aquest mètode que s'assegura que les dades siguin del mateix tipus en totes els mètodes que s'encarreguin de fer operacions.\n",
        "\n",
        "\n",
        "Per solucionar la següent qüestió, el que farem és fer ús dels operadors reflectants, bàsicament, es cridarà al mateix mètode de l'operació que es desitgi fer, però invertint l'ordre de les dades que es passen com a paràmetre.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CwHRgxbSAkqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _ensureVariable(x):\n",
        "  if isinstance(x, Variable):\n",
        "      return x\n",
        "  elif isinstance(x, int) or isinstance(x, float):\n",
        "      return Variable(x)\n",
        "  else:\n",
        "      raise TypeError(f\"No es pot convertir la variable de tipus {type(x)} a <class '__main__.Variable>.\")"
      ],
      "metadata": {
        "id": "bHJUcAsdBS1a"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vAdd(A: Variable, B: Variable): # Addition\n",
        "    A = _ensureVariable(A)\n",
        "    B = _ensureVariable(B)\n",
        "  \n",
        "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
        "    \n",
        "    if (A.requiresGrad):\n",
        "      #keep track of the parent variables, and of the local derivative associated with each one\n",
        "      result.requiresGrad = True\n",
        "      result.gradRoutes.append((A, 1)) # dresult / dA = 1\n",
        "\n",
        "    if (B.requiresGrad):\n",
        "      result.requiresGrad = True\n",
        "      result.gradRoutes.append((B, 1)) # dresult / dB = 1\n",
        "    \n",
        "    return result\n",
        "    \n",
        "def vMul(A: Variable, B: Variable): # Addition\n",
        "    A = _ensureVariable(A)\n",
        "    B = _ensureVariable(B)\n",
        "\n",
        "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a * b\n",
        "\n",
        "    if (A.requiresGrad):    \n",
        "      #keep track of the parent variables, and of the local derivative associated with each one\n",
        "      result.requiresGrad = True\n",
        "      result.gradRoutes.append((A, B.value)) # dresult / dA = B\n",
        "\n",
        "    if (B.requiresGrad):\n",
        "      result.requiresGrad = True\n",
        "      result.gradRoutes.append((B, A.value)) # dresult / dB = A\n",
        "    \n",
        "    return result\n",
        "  \n",
        "\n",
        "def vPow(A: Variable, exponent: float):\n",
        "    A = _ensureVariable(A)\n",
        "    exponent = _ensureVariable(exponent)\n",
        "\n",
        "    result = Variable(np.float_power(A.value, exponent))\n",
        "\n",
        "    if (A.requiresGrad):   \n",
        "      result.requiresGrad = True\n",
        "      result.grad = 0.0\n",
        "      result.gradRoutes.append((A, exponent*np.float_power(A.value, exponent -1)))\n",
        "    \n",
        "\n",
        "    return result\n",
        "\n",
        "def vExp(A: Variable):\n",
        "    A = _ensureVariable(A)\n",
        "\n",
        "    result = Variable(np.exp(A.value))\n",
        "\n",
        "    if (A.requiresGrad):   \n",
        "      result.requiresGrad = True\n",
        "      result.grad = 0.0\n",
        "      result.gradRoutes.append((A, np.exp(A.value)))\n",
        "\n",
        "    return result\n",
        "\n",
        "def vLog(A: Variable):\n",
        "    A = _ensureVariable(A)\n",
        "\n",
        "    result = Variable(np.log(A.value))\n",
        "\n",
        "    if (A.requiresGrad):   \n",
        "      result.requiresGrad = True\n",
        "      result.grad = 0.0\n",
        "      result.gradRoutes.append((A, 1/A.value))\n",
        "\n",
        "def vNeg(A: Variable):\n",
        "    A = _ensureVariable(A)\n",
        "\n",
        "    result = Variable(A.value * (-1))\n",
        "\n",
        "    if (A.requiresGrad):   \n",
        "      result.requiresGrad = True\n",
        "      result.grad = 0.0\n",
        "      result.gradRoutes.append((A, A.value * (-1)))"
      ],
      "metadata": {
        "id": "Q8JaHiWMB8kD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OG4h-oTaNZaR"
      },
      "outputs": [],
      "source": [
        "# Your Code Here\n",
        "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
        "    def __init__(self, value, requiresGrad = False):\n",
        "        self.value = value\n",
        "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
        "        self.grad = 0.0\n",
        "        self.requiresGrad = requiresGrad\n",
        "    \n",
        "    def backProp(self, route_val = 1.0):\n",
        "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
        "        self.grad += route_val\n",
        "                \n",
        "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
        "        for variable, local_derivative_value in self.gradRoutes:\n",
        "            # Multiply the incoming gradient with the local derivatives corresponding the parent variable, and continue the backpropagation\n",
        "            variable.backProp(local_derivative_value * route_val)\n",
        "    \n",
        "    def zeroGrad(self):\n",
        "        self.grad = 0.0\n",
        "        \n",
        "    def zeroGradsRecursively(self):\n",
        "        self.zeroGrad()\n",
        "        for variable, _ in self.gradRoutes:\n",
        "            variable.zeroGradsRecursively()\n",
        "            \n",
        "    def __add__(self, b):\n",
        "        return vAdd(self, b)\n",
        "    \n",
        "    def __radd__(self, b):\n",
        "        return vAdd(b, self)\n",
        "    \n",
        "    def __sub__(self, b):\n",
        "        return vAdd(self, vMul(b, Variable(-1.0)))\n",
        "    \n",
        "    def __rsub__(self, b):\n",
        "        return vAdd(b, -1 * self)\n",
        "  \n",
        "    def __mul__(self, b):\n",
        "        return vMul(self, b)       \n",
        "    \n",
        "    def __rmul__(self, b):\n",
        "        return vMul(b, self)\n",
        "\n",
        "    def __pow__(self, exponent):\n",
        "        return vPow(self, exponent)\n",
        "\n",
        "    def __rpow__(self, exponent):\n",
        "        return vPow(exponent, self)\n",
        "                    \n",
        "    def __truediv__(self, b):\n",
        "        return vMul(self, b**(-1.0))\n",
        "    \n",
        "    def __rtruediv__(self, b):\n",
        "        return vMul(b, self**(-1.0))\n",
        "    \n",
        "    def __neg__(self):\n",
        "        return self * Variable(-1)\n",
        "    \n",
        "    def __rneg__(self):\n",
        "        return Variable(-1) * self\n",
        "\n",
        "    def __str__(self):\n",
        "        if self.requiresGrad:\n",
        "          return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)\n",
        "        else:\n",
        "          return 'Value: {self.value}, Gradient not required'.format(self=self)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprovem si funciona!"
      ],
      "metadata": {
        "id": "ERGosFgRCikh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = Variable(3)\n",
        "\n",
        "a = x * 2\n",
        "b = x + 2\n",
        "c = x - 2\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c, '\\n')\n",
        "\n",
        "# Reflected\n",
        "a = 2 * x\n",
        "b = 2 + x\n",
        "c = 2 - x\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vymBrofA3Wt",
        "outputId": "7dbe4880-7474-4e66-c8ff-dc40a83ab90b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value: 6, Gradient not required\n",
            "Value: 5, Gradient not required\n",
            "Value: 1.0, Gradient not required \n",
            "\n",
            "Value: 6, Gradient not required\n",
            "Value: 5, Gradient not required\n",
            "Value: -1, Gradient not required\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFD25Z5dNZaS"
      },
      "source": [
        "---\n",
        "### <font color=blue>Excercise 4:</font>\n",
        "    \n",
        "<font color=blue>Instead of keeping track of the local derivative value and doing the multiplication between the local derivative value and the incoming gradient explicitly, it is better to keep track of the actual function used to calculate the local derivative and combine it with the incoming one (the `route_val`). The idea is to keep a note of the recipe for calculating the local derivative value instead of the value itself.</font>\n",
        "\n",
        "<font color=blue>In our case, since we deal with scalars, this function would basically boil down to doing this multiplication with the `route_val` (applying the chain rule), but when we vectorise our inputs and start dealing with tensors instead of scalars (and `route_val` becomes a matrix), this will become a bit more complicated (a dot product). So, keeping track of the function instead of the local derivative value will allow us to easily extend this framework to this scenario.</font>\n",
        "\n",
        "<font color=blue>In addition, keeping a note of the function instead of the value, allows us to abstract away stuff. This basically means that we can build the computation graph first, with placeholder variables independently of specific input values, and then reuse it for different inputs. This is how many deep learning frameworks work.</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _ensureVariable(x):\n",
        "  if isinstance(x, Variable):\n",
        "      return x\n",
        "  elif isinstance(x, int) or isinstance(x, float):\n",
        "      return Variable(x)\n",
        "  else:\n",
        "      raise TypeError(f\"No es pot convertir la variable de tipus {type(x)} a <class '__main__.Variable>.\")"
      ],
      "metadata": {
        "id": "uKgjbYOKDLLQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vAdd(A, B): # Addition\n",
        "    A = _ensureVariable(A)\n",
        "    B = _ensureVariable(B)\n",
        "    result = Variable(A.value + B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
        "          \n",
        "    #keep track of the parent variables, and of the local derivative associated with each one\n",
        "    if A.requiresGrad:\n",
        "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
        "        def gradfn(route_val):\n",
        "            return route_val # == route_val * 1, as here dresult / dA = 1\n",
        "        result.gradRoutes.append((A, gradfn)) \n",
        "    if B.requiresGrad:\n",
        "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
        "        def gradfn(route_val):\n",
        "            return route_val # == route_val * 1, as here dresult / dA = 1\n",
        "        result.gradRoutes.append((B, gradfn))\n",
        "    \n",
        "    return result\n",
        "    \n",
        "def vMul(A, B): # Addition\n",
        "    A = _ensureVariable(A)\n",
        "    B = _ensureVariable(B)    \n",
        "    result = Variable(A.value * B.value) # Create a new Variable to store the result, and pass it the value = a + b\n",
        "          \n",
        "    #keep track of the parent variables, and of the local derivative associated with each one\n",
        "    if A.requiresGrad:\n",
        "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
        "        def gradfn(route_val):\n",
        "            return route_val * B.value # dresult / dA = B\n",
        "        result.gradRoutes.append((A, gradfn)) \n",
        "    if B.requiresGrad:\n",
        "        result.requiresGrad = True; # if any of the parent variables requires grad, the result should also require it\n",
        "        def gradfn(route_val):\n",
        "            return route_val * A.value # dresult / dB = A\n",
        "        result.gradRoutes.append((B, gradfn)) \n",
        "    \n",
        "    return result\n",
        "\n",
        "#Pointwise raise to power\n",
        "def vPow(A, exponent: float):\n",
        "    A = _ensureVariable(A)\n",
        "    result = Variable(np.float_power(A.value, exponent))\n",
        "    \n",
        "    if A.requiresGrad:\n",
        "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
        "        result.grad = 0.0 # initialise to zero\n",
        "        def gradfn(route_val):\n",
        "            return route_val * exponent*np.float_power(A.value, exponent-1)\n",
        "        result.gradRoutes.append((A, gradfn))\n",
        "        \n",
        "    return result\n",
        "    \n",
        "#Pointwise exp()\n",
        "def vExp(A):\n",
        "    A = _ensureVariable(A)\n",
        "    result = Variable(np.exp(A.value))\n",
        "    \n",
        "    if A.requiresGrad:\n",
        "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
        "        result.grad = 0.0 # initialise to zero\n",
        "        def gradfn(route_val):\n",
        "            return route_val * np.exp(A.value)\n",
        "        result.gradRoutes.append((A, gradfn))\n",
        "\n",
        "    return result\n",
        "                    \n",
        "#Pointwise Log\n",
        "def vLog(A):\n",
        "    A = _ensureVariable(A)\n",
        "    result = Variable(np.log(A.value))\n",
        "    \n",
        "    if A.requiresGrad:\n",
        "        result.requiresGrad = True; # if any of our dependencies requires grad, we should also require it\n",
        "        result.grad = 0.0 # initialise to zero\n",
        "        def gradfn(route_val):\n",
        "            return route_val/A.value\n",
        "        result.gradRoutes.append((A, gradfn))\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "EqfpslbACXI3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Variable: #Simple variable. These are the leafs of our tree, they can request to have a gradient calculated, or not\n",
        "    def __init__(self, value, requiresGrad = False):\n",
        "        self.value = value\n",
        "        self.gradRoutes = [] # A variable by default has no grad Routes (was not created by anything, just defined)\n",
        "        self.grad = 0.0\n",
        "        self.requiresGrad = requiresGrad\n",
        "    \n",
        "    def backProp(self, route_val = 1.0):\n",
        "        # Add together the incoming gradients from the different routes that lead to a node - this will be the final gradient for the node\n",
        "        self.grad += route_val\n",
        "                \n",
        "        # For every parent variable and corresponding local derivative value that we have in the gradRoutes, continue with the gradient calculation\n",
        "        for variable, gradfn in self.gradRoutes:\n",
        "            # Call the gradfn() of each of the variables in our gradRouts passing the incoming gradient, and continue the backpropagation with the returned value\n",
        "            variable.backProp(gradfn(route_val))\n",
        "\n",
        "    def zeroGrad(self):\n",
        "        self.grad = 0.0\n",
        "        \n",
        "    def zeroGradsRecursively(self):\n",
        "        self.zeroGrad()\n",
        "        for variable, _ in self.gradRoutes:\n",
        "            variable.zeroGradsRecursively()\n",
        "            \n",
        "    def __add__(self, b):\n",
        "        return vAdd(self, b)\n",
        "\n",
        "    def __radd__(self, b):\n",
        "        return vAdd(self, b)\n",
        "    \n",
        "    def __sub__(self, b):\n",
        "        return vAdd(self, vMul(b, Variable(-1.0)))\n",
        "\n",
        "    def __rsub__(self, b):\n",
        "        return vAdd(b, -1 * self)\n",
        "    \n",
        "    def __mul__(self, b):\n",
        "        return vMul(self, b)\n",
        "    \n",
        "    def __rmul__(self, b): # BE CAREFUL WITH THIS ONE. While we deal with scalars, order is not a problem, when we deal with matrices, this will\n",
        "        return vMul(self, b)\n",
        "    \n",
        "    def __pow__(self, exponent):\n",
        "        return vPow(self, exponent)\n",
        "    \n",
        "    def __truediv__(self, b):\n",
        "        return vMul(self, pow(b, -1.0))\n",
        "    \n",
        "    def __rtruediv__(self, b):\n",
        "        return vMul(b, pow(self, -1.0))\n",
        "    \n",
        "    def __neg__(self):\n",
        "        return -1 * self\n",
        " \n",
        "    def __str__(self):\n",
        "        if self.requiresGrad:\n",
        "            return 'Value: {self.value}, Gradient: {self.grad}'.format(self=self)\n",
        "        else:\n",
        "            return 'Value: {self.value}, Gradient not required'.format(self=self)"
      ],
      "metadata": {
        "id": "PfWJJssfCiwO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk9Af8UXNZaS"
      },
      "source": [
        "---\n",
        "### <font color=blue>Excercise 5:</font>\n",
        "    \n",
        "<font color=blue>Write some code to manually check that your gradient calculation is correct, using the property of:</font>\n",
        "\n",
        "$$\n",
        "f'(x) = \\frac {f(x+\\epsilon) - f(x-\\epsilon)}{2 \\epsilon}\n",
        "$$\n",
        "\n",
        "<font color=blue>where $\\epsilon$ is a very small number to approximately calculate the gradient. Then use it to calculate the derivative of the function $f(x) = 21 * x^3$ at $x=3.2$. Double check that our framework gives you the same result.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primerament, fem la derivada teòrica, aplicant la propietat que ens proporciona l'enunciat. Notem que la funció ex5() que hem creat és la funció *f()* que ens diu l'enunciat. Veiem que segons aquesta propietat la derivada de la funció respecte x és d'aproximadament 645,12."
      ],
      "metadata": {
        "id": "ISs4PSU4V_io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ex5(x):\n",
        "    return 21*(x**3)\n",
        "    \n",
        "epsilon = 0.1*10**(-10)\n",
        "x = 3.2\n",
        "gradfn_teoric = (ex5(x + epsilon) - ex5(x - epsilon)) / (2 * epsilon)\n",
        "print(gradfn_teoric)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYpCDG-kC1R2",
        "outputId": "c5ab3b17-81cf-4a1b-f657-8216c897a8a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "645.1216449931961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ara fem ús del nostre framework. Primerament, creem l'objecte Variable del nostre punt x. Després propaguem enrere la funció amb *backProp()*, notem que aquesta funció farà servir les operacions (mul, sub, add, etc) que hem definit en els passos anteriors.\n",
        "\n",
        "Un cop tenim això, calculem el gradient respecte al punt x i tenim un resultat molt similar a l'anterior."
      ],
      "metadata": {
        "id": "M0YfXAaYWVrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_framework = Variable(3.2, requiresGrad = True)\n",
        "res = (21 * (x_framework**3))\n",
        "res.backProp()\n",
        "grafn_framework = x_framework.grad\n",
        "print(grafn_framework)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxWGmYKsGkhO",
        "outputId": "eddbc2cf-c703-49d3-9746-c75049e376c1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "645.1200000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"L'error és de: {round(abs(gradfn_teoric - grafn_framework), 5)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpMt9qzDW7Qg",
        "outputId": "849668db-ef28-41a0-a5fc-6860ca80e6e7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L'error és de: 0.00164.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per tant, podem afirmar que el nostre framework funciona bé i és bastant precís."
      ],
      "metadata": {
        "id": "n9AzrqQCXJ0D"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "XN_ED_Lab01_01_AutoGrad_onScalars.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
